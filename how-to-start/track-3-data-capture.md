# ğŸ•µï¸ Track 3 (Data Capture)


This track focuses on the actual capture of at-risk data in a variety of formats. As these tasks require the most technical knowledge, skills, and equipment, volunteers are encouraged to take this track when they are able to dedicate more time. The track has been adapted for asynchronous work, if you have any questions please submit your question to datarescue@wustl.libanswers.com.

Please complete this [anonymous participation survey](https://wustl.az1.qualtrics.com/jfe/form/SV_2reEOCtWu6Niowm) before starting this track.

**Tech Skill Level:** Advanced

**Time Commitment:** \~2-3 hours

**Tasks Include:**

0. [LIMITED TIME RESCUE] Work RDAP to Save NCES Data - High Priority
1. Harvest public datasets
2. Add metadata 
3. Organize and package data for long-term storage
4. Investigate and nominate at risk data

**Tools Required (vary across tasks):**

* Spreadsheet editor (i.e., excel, google sheets)
* Storage: [https://wustl.box.com/s/cjboayt41bi202zb3ddq44q5jgrmd2ti](https://wustl.box.com/s/kenp427zu5to1kjkhkwsh3buhi8ifr2q)
  * Large file transfer (wustl key only): https://app.globus.org/file-manager/collections/6ebc17da-232a-4bec-97fb-eb6c03e8c1fe


**Breakdown of Task Sections**\
ğŸš _(helicopter emoji)_ gives summary of task\
ğŸ—‚ï¸ _(index dividers)_ outlines specific steps needed to complete task\
ğŸ› ï¸ _(hammer & wrench emoji)_ details skills & tools needed for task\
ğŸ’ _(information desk person)_ details participant role

### TASKS BREAKDOWN

#### <mark style="background-color:purple;"> [LIMITED TIME RESCUE] Work with RDAP to Rescue data from NCES</mark>

ğŸš**Summary:** The Research Data Access and Preservation(RDAP) Association is organizing data rescue efforts to capture data from the National Center for Education Statistics (NCES). Since 1867, NCES has been the federal statistical agency responsible for collecting, analyzing, and reporting data on the condition of U.S. educationâ€”from early childhood to adult educationâ€”to help improve student outcomes. The data in this workflow has been deemed as high priority and at risk for removal.

ğŸ’**Role:** Data Collection Collaborator

ğŸ—‚ï¸**Workflow**


1. Navigate to the instructions created by RDAP: [RDAP Instructions](https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fdocs.google.com%2Fdocument%2Fd%2F1VWp6ZZSsYTG87j_3PaWV5df7S5cy3tyu2MW94gZLH3U%2Fedit%3Fusp%3Dsharing&data=05%7C02%7Cavianna%40wustl.edu%7Ce5c9085810bc4de0466908dd8f0bd15c%7C4ccca3b571cd4e6d974b4d9beb96c6d6%7C0%7C0%7C638824003129250179%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=s%2B73JqEaoew1lnNWuU9BzfayyRhx74MJmjSbPj3imG4%3D&reserved=0)
2. Please note that each table number for the [RDAP tracker](https://docs.google.com/spreadsheets/d/19Q4Fus033NZrjAYG9RuSlAs3nimMq9UHUO4c_Eh_a28/edit?usp=sharing) has multiple datasets associated with said number. While there are 123 data table numbers, there are more than 123 datasets. 
   * See Step 5 in the [RDAP Instructions](https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fdocs.google.com%2Fdocument%2Fd%2F1VWp6ZZSsYTG87j_3PaWV5df7S5cy3tyu2MW94gZLH3U%2Fedit%3Fusp%3Dsharing&data=05%7C02%7Cavianna%40wustl.edu%7Ce5c9085810bc4de0466908dd8f0bd15c%7C4ccca3b571cd4e6d974b4d9beb96c6d6%7C0%7C0%7C638824003129250179%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=s%2B73JqEaoew1lnNWuU9BzfayyRhx74MJmjSbPj3imG4%3D&reserved=0) for further clarification.
4. If you have any questions about this workflow, please email datarescue@wustl.libanswers.com. 

ğŸ› ï¸**Skills Needed:** Intermediate understanding of metadata and different dataset types. Comfort with cataloging metadata for datasets.&#x20;

#### <mark style="background-color:purple;">1. Harvest public datasets available online</mark>

ğŸš**Summary:** Some state and federal agencies are required by law to publish data, publications, and basic information about publicly funded projects (think grants and contracts). Given changes in agency personnel, system updates, as well as financial support to pay for database services and storage, the data stored in these repositories may not always be available for the public. Saving copies can help ensure future access as well as information on past government activities and areas of interests.

ğŸ’**Role:** Data Collector

ğŸ—‚ï¸**Workflow**

1. Navigate to the [WashU Data Rescue Tracker](https://app.smartsheet.com/b/publish?EQBCT=e36fff078be14a2f826b0b804fd9d5f4) 
2. Each row in the smartsheet corresponds to a dataset. Claim a dataset in the sheet that has not yet been Claimed, indicated by a blank in the column â€œClaimed By (email address).â€ 
      * To Claim a dataset type in your email in the Claimed By coloumn and press enter. The dropdown will only work for those who have already entered their email address. 
      * Claimed datasets will be highlighted in yellow.
3. Update the "Data Downloaded" column to â€œin processâ€ to indicate you are working on it. 
4.	Click on the provided URL and orient yourself to the structure of the landing page for the dataset.
5.	Identify downloadable data files and documentation files. Verify that the files include adequate descriptive information, (e.g., READMEs, clear file names, data dictionaries, etc.).
7.	Take a screenshot of the dataset's webpage to document provenance of the data for future users of the dataset. Include the screenshot with the files you downloaded. 
8.	Capture dataset file(s) to internal device storage and place all files in a folder with an identifying name. If downloading the files for the dataset is difficult or impossible, update the â€œDataset Download Possible?â€ column to â€œNoâ€ and leave a note about the difficulty in the â€œNotesâ€ column. Otherwise, update â€œDataset Download Possible?â€ column to â€œYesâ€.
9.	Update the â€œData Downloadedâ€ column to â€œYesâ€. Proceed to Workflow 2 ("Add Metadata..."). Please email [datarescue@wustl.libanswers.com](datarescue@wustl.libanswers.com) if you have any issues. 

ğŸ› ï¸**Skills Needed:** Intermediate understanding of metadata and different dataset types. Comfort with cataloging metadata for datasets.&#x20;


#### <mark style="background-color:purple;">2. Add metadata to harvested datasets</mark>

ğŸš**Summary:** Adding metadata to rescued data is a crucial step in making the data findable for future use. Metadata is often described as data about data. This workflow will focus entirely on adding information about the data we have saved thus far in our data rescue efforts. 

ğŸ’**Role:** Data Archivist

ğŸ—‚ï¸**Workflow**

1. Navigate to the [WashU Data Rescue Tracker](https://app.smartsheet.com/sheets/h5xJW8m5v6xPX5w2qx8f2qqg7CW9xXr3r3qcQhq1) 
2. Go to the row applicable for your selected dataset.
3. Click the URL listed for your dataset and navigate to the federal web page housing the dataset.
4. Review the information on the webpage that describes the dataset.
5. Update as many of the columns as you can. However, not all of the columns will be applicable for each dataset. Columns like Data range or geographical coverage may not apply to the dataset.
6. If you have any quesions submit them to datarescue@wustl.libanswers.com.
7. Move on to workflow 3 (Organize and package data for long-term storage).

ğŸ› ï¸**Skills Needed:** Intermediate understanding of different dataset types and file formats. Comfort with downloading and saving larger files.

#### <mark style="background-color:purple;">3. Organize and package data for long-term storage</mark>

ğŸš**Summary:** For long-term usability, data files should be organized logically and descriptively to enable future use without the barrier of figuring out what each file contains. For consistent ecological sustainability, large files or groups of files should be compressed to minimize the quantity to digital space and computational power they require to store. In preparation for the datasetsâ€™ transfer to secured third-party government data repositories, organize and package your downloaded data into an ideal format.

ğŸ’**Role:** Digital Preservationist

ğŸ—‚ï¸**Workflow**

1.	Group files into nested folders with logical subdivisions (e.g., separating data files from documentation files, group data files by locally           important variables like years or geographic focus, etc.).
2.	Create a folder in the [WashU Data Rescue Box](https://wustl.box.com/s/kenp427zu5to1kjkhkwsh3buhi8ifr2q) where you can put your transfer. The          folder naming convention is: data_transfer_yourinitials
4.	Navigate to the top folder for the dataset you saved. Right click and choose â€œCompress toâ€¦â€ then ZIP File. 
5.	Drop the zipped file into the [Box](https://wustl.box.com/s/kenp427zu5to1kjkhkwsh3buhi8ifr2q) folder you created in step 2.
6.	If you cannot access the Box foder, send an email to datarescue@wustl.libanswers.com, subject line: â€œWashU Data Rescue Transfer [Date]â€ with the       date in MM-DD-YYYY formatting.
   * If possible, attach the compressed ZIP file you created in step 2 to the email.
   * If files are too large to be attached, add the title of the dataset, and file size information in your email. 
7.	If files are too large to put in box, and you have an active WUSTL key you can request access to transfer via Globus:
  * for access, navigate to [Globus transfer application](https://app.globus.org/file-manager/collections/6ebc17da-232a-4bec-97fb-eb6c03e8c1fe), where     you will be prompted through steps to receive transfer permissions.
  * if you have trouble, email datarescue@wustl.libanswers.com to determine other means of transfer.
8. If files are too large for Box, and you DO NOT have an active WUSTL key:
  * Email datarescue@wustl.libanswers.com to determine other means of transfer. 
9.	In the â€œTransferred to Coordinator?â€ column of the [WashU Data Rescue Tracker](https://app.smartsheet.com/sheets/h5xJW8m5v6xPX5w2qx8f2qqg7CW9xXr3r3qcQhq1), update the  column for your dataset to â€œYâ€.
10.	After receiving confirmation that the file transfer was successful, delete the dataset from internal device storage.


ğŸ› ï¸**Skills Needed:** Basic understanding of file compression. Comfort applying principles of information organization and taxonomic structures.

#### <mark style="background-color:purple;">4. Research and identify at-risk datasets</mark>

ğŸš**Summary:** As the situation regarding the dismantling of federal agencies evolves, it is important the nation-wide data rescue movement continues to proactively identify datasets for preservation. Through a combination of media research and webpage exploration, data rescue volunteers find at-risk, publically accessible datasets for our local Dataset Archiving Tracker. Avoiding duplicative effort in data rescue is crucial to maximize the output of this volunteer movement, which makes double-checking the main repositories and national data rescue trackers an imperative step in this task.

ğŸ’**Role:** Dataset Investigator

ğŸ—‚ï¸**Workflow**

1.	Conduct brief research into the current state of federal agencies that are being targetted or are rumored to be targetted for depletions in workforce or budget. Research avenues include"published on social media: 
      * Review recent statements published on social media made by federal government officials associated with the Department of Government Efficiency (DOGE)
      * Search public messaging sites, (e.g., BlueSky, X (formerly Twitter), etc.), with keyword terms related to "data rescue" or "federal data" for any up-to-date advice on data rescue targets
      * Skim reliable news outlets for any reports of data taken offline in recent days or updates on current DOGE efforts
      * If finding an agency to investigate proves difficult, consult WashU Data Rescue coordinators to develop a research strategy
2.	With an agency identified, navigate to the federal government website associated with that agency; start from [usa.gov's Agency Index](https://www.usa.gov/agency-index) to begin with the agency's homepage. Explore the website for any webpages that may house public datasets. Carefully consider hyperlinks to webpages containing words like:
      * Data
      * Reports
      * Communications
      * Finances
      * Publications
      * Research
      * Policy
      * Archive 
3.	Once you have identified a dataset, check the following to ensure it has not already been harvested:
      * [DRP's Data Rescue Tracker](https://baserow.datarescueproject.org/public/grid/Nt_M6errAkVRIc3NZmdM8wcl74n9tFKaDLrr831kIn4)
      * If environmental data, [EDGI's Federal Environmental Database Priority List](https://docs.google.com/spreadsheets/d/1ZHUKy0fmG6AfOYmRFrdAfXsvHWtQojrjwMHU1va7k_A/edit?gid=1202364901#gid=1202364901)
      * [ICPSR's DataLumos Archive](https://datalumos.org/)
      * If you have prior knowledge in the area of research data, any other relevant third-party public data repositories 
4.	If dataset has not been found in any of the above data rescue efforts, add metadata about the dataset as a new line in the Dataset Archiving sheet of the [WashU Data Rescue Tracker](https://app.smartsheet.com/sheets/h5xJW8m5v6xPX5w2qx8f2qqg7CW9xXr3r3qcQhq1). Fill out as many of the fields row as possible, but at minimum provide the Agency, Title of Dataset, and URL.
5.	In the "Data Downloaded" column place an "N". Place any relevant notes in the "Final Notes" column.
6.	If you choose to continue with the harvesting, description, and organization of the added dataset, proceed to track 3, task 1 (above) and complete tasks 1-3. Otherwise continue investigation of previously identified agency website for more at-risk datasets, returning to step 2 of this task.


ğŸ› ï¸**Skills Needed:** Comfort conducting internet and social media research. Confidence searching repository catalogs. Desire to proactively leave "no stone unturned" in the ongoing protection of public federal datasets.


