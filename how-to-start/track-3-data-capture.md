# 🕵️ Track 3 (Data Capture)


This track focuses on the actual capture of at-risk data in a variety of formats. As these tasks require the most technical knowledge, skills, and equipment, volunteers are encouraged to take this track when they are able to dedicate more time.

**Tech Skill Level:** Advanced

**Time Commitment:** \~2-3 hours

**Tasks Include:**

1. Harvest public datasets
2. Add Metadata 
3. Organize & Package Data for Long-Term Storage

**Tools Required (vary across tasks):**

* Spreadsheet editor (i.e., excel, google sheets)
* Storage (available internal memory, external hard drive)


**Breakdown of Task Sections**\
🚁 _(helicopter emoji)_ gives summary of task\
🗂️ _(index dividers)_ outlines specific steps needed to complete task\
🛠️ _(hammer & wrench emoji)_ details skills & tools needed for task\
💁 _(information desk person)_ details participant role

### TASKS BREAKDOWN

#### <mark style="background-color:purple;">1. Harvest Public Datasets Available Online</mark>

🚁**Summary:** Some state and federal agencies are required by law to publish data, publications, and basic information about publicly funded projects (think grants and contracts) Given changes in agency personnel, system updates, as well as financial support to pay for database services and storage, the data stored in these repositories may not always be available for the public. Saving copies can help ensure future access as well as information on past government activities and areas of interests.

💁**Role:** Data Collector

🗂️**Workflow**

1. Search for publicly funded project repositories (examples include: NIH [RePORTER](https://reporter.nih.gov/), US Government Awards [USASpending](https://www.usaspending.gov/search), Federal Audit Clearinghouse [FAC](https://app.fac.gov/dissemination/search/), [NWIS - National Water Information System](https://waterdata.usgs.gov/nwis?) and many others)
2. Verify that downloadable datasets contain enough descriptive information (data files, interactive maps, etc.)&#x20;
3. Capture dataset(s) to internal storage (temporary place)
4. Submit and upload the dataset(s) via 1 of these options
   * Files up to 2 GB [https://wetransfer.com/](https://wetransfer.com/)&#x20;
   * OR submit the URL of a downloadable folder via the exit tix [**link to Work Completion Form**]&#x20;
5. You can delete dataset after successful transfer to Data Rescue coordinators

🛠️**Skills Needed:** Intermediate understanding of software deployment and website navigation.&#x20;

#### <mark style="background-color:purple;">2. Add Metadata to Harvested Datasets</mark>

🚁**Summary:** Adding metadata to rescued data is a crucial step in making the data findable for future use. Metadata is often described as data about data. This workflow will focus entirely on adding  information about the data we have saved thus far in our data rescue efforts. 

💁**Role:** Data Archivist

🗂️**Workflow**

1. Search for publicly funded project repositories (examples include: NIH [RePORTER](https://reporter.nih.gov/), US Government Awards [USASpending](https://www.usaspending.gov/search), Federal Audit Clearinghouse [FAC](https://app.fac.gov/dissemination/search/), [NWIS - National Water Information System](https://waterdata.usgs.gov/nwis?) and many others)
2. Verify that downloadable datasets contain enough descriptive information (data files, interactive maps, etc.)&#x20;
3. Capture dataset(s) to internal storage (temporary place)
4. Submit and upload the dataset(s) via 1 of these options
   * Files up to 2 GB [https://wetransfer.com/](https://wetransfer.com/)&#x20;
   * OR submit the URL of a downloadable folder via the exit tix [**link to Work Completion Form**]&#x20;
5. You can delete dataset after successful transfer to Data Rescue coordinators

🛠️**Skills Needed:** Intermediate understanding of different dataset types and file formats. Comfort with downloading and saving larger files.

#### <mark style="background-color:purple;">3. Organize & Package Data for Long-Term Storage</mark>

🚁**Summary:** This helps short and long term preservation effort to verify the integrity (fixity) of stored files and datasets. Creating checksums or reviewing them helps detect transfer or creation errors or signs of tampering by external forces.

💁**Role:** Digital Preservationist

🗂️**Workflow**

* Read through the [digital preservation manual chapter on fixity and checksums by the Digital Preservation Coalition](https://www.dpconline.org/handbook/technical-solutions-and-tools/fixity-and-checksums)&#x20;
* Download a fixity or checksum verification tool like
  * [Md5summer](https://md5summer.org/): An application for Windows machines that will generate and verify md5 checksums.
  * [checksum](https://corz.org/windows/software/checksum/): A file hashing application for Windows, a program that generates and verifies BLAKE2, SHA1 and MD5 hashes (aka. "MD5 Sums", or "digital fingerprints") of a file, a folder, or recursively.
  * There are a number of other tools, the above mentioned are examples (see Digital Preservation Coalition Digital Preservation Handbook above).
* Ask the "data titan" coordinator to gain access to captured files
* Use the [**link to Data Tracking List**] to check details to create checksum&#x20;
* Run a check on the selected data to create the supplemental checksum value
* Upload checksum file using (1 )of the following options
    * Submit and upload the dataset(s) via 1 of these options
      * Files up to 2 GB [https://wetransfer.com/](https://wetransfer.com/)&#x20;
      * OR submit the URL of a downloadable folder via the exit tix [**link to Work Completion Form**]&#x20;

🛠️**Skills Needed:** Best for those who have strong tech skills, attention to detail, and willingness to read the docs.


