# 🕵️ Track 3 (Data Capture)


This track focuses on the actual capture of at-risk data in a variety of formats. As these tasks require the most technical knowledge, skills, and equipment, volunteers are encouraged to take this track when they are able to dedicate more time.

**Tech Skill Level:** Advanced

**Time Commitment:** \~2-3 hours

**Tasks Include:**

1. Harvest Public Datasets
2. Add Metadata 
3. Organize & Package Data for Long-Term Storage

**Tools Required (vary across tasks):**

* Spreadsheet editor (i.e., excel, google sheets)
* Storage (available internal memory, external hard drive)


**Breakdown of Task Sections**\
🚁 _(helicopter emoji)_ gives summary of task\
🗂️ _(index dividers)_ outlines specific steps needed to complete task\
🛠️ _(hammer & wrench emoji)_ details skills & tools needed for task\
💁 _(information desk person)_ details participant role

### TASKS BREAKDOWN

#### <mark style="background-color:purple;">1. Harvest Public Datasets Available Online</mark>

🚁**Summary:** Some state and federal agencies are required by law to publish data, publications, and basic information about publicly funded projects (think grants and contracts) Given changes in agency personnel, system updates, as well as financial support to pay for database services and storage, the data stored in these repositories may not always be available for the public. Saving copies can help ensure future access as well as information on past government activities and areas of interests.

💁**Role:** Data Collector

🗂️**Workflow**

1.	Navigate to the Dataset Archiving Tracker and choose a dataset that has not yet been downloaded, indicated by an “N” in the column “Data Downloaded” within the “File Transfer Checklist” section. Update the column to IP for “in process” to indicate you are working on it. Add your name to the cell in the “Claimed by” column.
2.	Click on the provided URL and orient yourself to the structure of the landing page for the dataset.
3.	Identify downloadable data files and documentation files. Verify that the files include adequate descriptive information, (e.g., READMEs, clear file names, data dictionaries, etc.) 
4.	Capture dataset file(s) to internal device storage and place all files in a folder with an identifying name. If downloading the files for the dataset is difficult or impossible, update the “Dataset Download Possible?” column to “N” and leave a note about the difficulty in the “Notes” column. Otherwise, update “Dataset Download Possible?” column to “Y”.
5.	Update the “Data Downloaded” column to “Y”. Proceed to Task 3.2 if time allows.

🛠️**Skills Needed:** Intermediate understanding of different dataset types and file formats. Comfort with downloading and saving larger files.&#x20;

#### <mark style="background-color:purple;">2. Add Metadata to Harvested Datasets</mark>

🚁**Summary:** Adding metadata to rescued data is a crucial step in making the data findable for future use. Metadata is often described as data about data. This workflow will focus entirely on adding  information about the data we have saved thus far in our data rescue efforts. 

💁**Role:** Data Archivist

🗂️**Workflow**

1. Search for publicly funded project repositories (examples include: NIH [RePORTER](https://reporter.nih.gov/), US Government Awards [USASpending](https://www.usaspending.gov/search), Federal Audit Clearinghouse [FAC](https://app.fac.gov/dissemination/search/), [NWIS - National Water Information System](https://waterdata.usgs.gov/nwis?) and many others)
2. Verify that downloadable datasets contain enough descriptive information (data files, interactive maps, etc.)&#x20;
3. Capture dataset(s) to internal storage (temporary place)
4. Submit and upload the dataset(s) via 1 of these options
   * Files up to 2 GB [https://wetransfer.com/](https://wetransfer.com/)&#x20;
   * OR submit the URL of a downloadable folder via the exit tix [**link to Work Completion Form**]&#x20;
5. You can delete dataset after successful transfer to Data Rescue coordinators

🛠️**Skills Needed:** Intermediate understanding of different dataset types and file formats. Comfort with downloading and saving larger files.

#### <mark style="background-color:purple;">3. Organize & Package Data for Long-Term Storage</mark>

🚁**Summary:** For long-term usability, data files should be organized logically and descriptively to enable future use without the barrier of figuring out what each file contains. For consistent ecological sustainability, large files or groups of files should be compressed to minimize the quantity to digital space and computational power they require to store. In preparation for the datasets’ transfer to secured third-party government data repositories, organize and package your downloaded data into an ideal format.

💁**Role:** Digital Preservationist

🗂️**Workflow**

1.	Group files into nested folders with logical subdivisions (e.g., separating data files from documentation files, group data files by locally important variables like years or geographic focus, etc.)
2.	Navigate to the top folder for the dataset. Right click and choose “Compress to…” then ZIP File. 
3.	Navigate to your email and compose a message to researchdata@wustl.edu. Write “WashU Data Rescue [DATE]” with the date in MM-DD-YYYY formatting. Attach the compressed ZIP file you created in step 2.
       * If the file size is too large to send over email but below 2 GB, use [https://wetransfer.com/](https://wetransfer.com/)&#x20; to email the file.
5.	In the File Transfer Checklist section of the Data Archiving Tracker, update the “Transferred to Coordinator?” column for your dataset to “Y”.
6.	After receiving confirmation that the file transfer was successful, delete the dataset from internal device storage.


🛠️**Skills Needed:** Basic understanding of file compression. Comfort applying principles of information organization and taxonomic structures.

